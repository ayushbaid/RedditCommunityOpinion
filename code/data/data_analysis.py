# -*- coding: utf-8 -*-
"""data-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RKjHEUT1uIYiaDQt2YffetZ0gaRCwlk1
"""

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
import nltk
import os
import glob
import plotly.graph_objects as go
# import cufflinks as cf
from textblob import TextBlob
import re
import numpy as np
import pandas as pd


def get_subreddit_data(subreddit: str):
    base_path = '../datasets'
    train_file_template = os.path.join(
        base_path, '{}_100000_train_part*.npy'.format(subreddit))
    val_file_template = os.path.join(
        base_path, '{}_100000_val_part*.npy'.format(subreddit))

    train_files = glob.glob(train_file_template)
    val_files = glob.glob(val_file_template)

    train_comments = []
    for file_name in train_files:
        train_comments += np.load(file_name)

    val_comments = []
    for file_name in val_files:
        val_comments += np.load(file_name)

    return train_comments, val_comments


# cf.go_offline()
# cf.set_config_file(offline=False, world_readable=True)


# def enable_plotly_in_cell():
#     import IPython
#     from plotly.offline import init_notebook_mode
#     display(IPython.core.display.HTML(
#         '''<script src="/static/components/requirejs/require.js"></script>'''))
#     init_notebook_mode(connected=False)


# Ankur
# PATH = "/content/drive/My Drive/Google Colab/wstm-project/"
# data = np.load(PATH + "5k/soccer.npy")

# Tarun
#PATH = "/content/drive/My Drive/CS_6240_Scrapping/data_5000/"
#data = np.load(PATH + "soccer_5000.npy")

subreddit = 'soccer'

train_comments, val_comments = get_subreddit_data(subreddit)

df = pd.DataFrame(train_comments, columns=['comments'])
df_val = pd.DataFrame(val_comments, columns=['comments'])

df_train.sample(5)


def __format_sentences(inp: str) -> str:
    # removing subreddit and user links
    clean_review = re.sub(r'/?(r|u)/\w+/?', '', inp)

    clean_review = re.sub('[^a-zA-Z]', ' ', clean_review)

    # convert to lower case
    clean_review = clean_review.lower()

    return clean_review


df['clean_comments'] = df['comments'].apply(__format_sentences)

df['polarity'] = df['clean_comments'].map(
    lambda text: TextBlob(text).sentiment.polarity)

df['comment_len'] = df['clean_comments'].astype(str).apply(len)

df['word_count'] = df['clean_comments'].apply(lambda x: len(str(x).split()))

# enable_plotly_in_cell()

plot = df['polarity'].iplot(
    kind='hist',
    bins=50,
    xTitle='polarity',
    linecolor='black',
    yTitle='count',
    title='Sentiment Polarity Distribution')
fig = plot.get_figure()
fig.savefig("polarity_{}.png".format(subreddit))

# enable_plotly_in_cell()

df['word_count'].iplot(
    kind='hist',
    xTitle='rating',
    linecolor='black',
    yTitle='count',
    title='Word Count Distribution')
fig = plot.get_figure()
fig.savefig('word_count_{}.png'.format(subreddit))

# enable_plotly_in_cell()

df['comment_len'].iplot(
    kind='hist',
    bins=100,
    xTitle='Comment length',
    linecolor='black',
    yTitle='count',
    title='Comment Text Length Distribution')
fig = plot.get_figure()
fig.savefig('comment_text_length_{}.png'.format(subreddit))


# enable_plotly_in_cell()

# directory = "/content/drive/My Drive/CS_6240_Scrapping/data_5000/"
# fig = go.Figure()
# for file in os.listdir(directory):
#     filename = directory+"/"+file
#     fn = "".join(file.split("_")[:-1])
#     data = np.load(filename)
#     df = pd.DataFrame(data, columns=['comments'])
#     df['clean_comments'] = df['comments'].apply(__format_sentences)
#     df['polarity'] = df['clean_comments'].map(
#         lambda text: TextBlob(text).sentiment.polarity)
#     fig.add_trace(go.Box(y=df['polarity'], boxmean="sd", name=fn))

# fig.update_layout(title_text="Sentiment Polarity Box Plot",
#                   yaxis_title='sentiment')
# fig.show()

# nltk.download('punkt')

# # raw data stats

# directory = "/content/drive/My Drive/CS_6240_Scrapping/data_5000/"

# names = []
# entries_ct = []
# sent_ct = []
# tok_ct = []
# vocab_list = []
# vocab_size = []
# for file in os.listdir(directory):
#     filename = directory+"/"+file
#     data = np.load(filename)
#     df = pd.DataFrame(data, columns=['comments'])
#     df['clean_comments'] = df['comments'].apply(__format_sentences)
#     df['sentences'] = df['comments'].map(lambda text: len(sent_tokenize(text)))
#     df['tokens_len'] = df['clean_comments'].map(
#         lambda text: len(word_tokenize(text)))
#     df['tokens'] = df['clean_comments'].map(lambda text: word_tokenize(text))
#     df['unique_tokens'] = df['tokens'].apply(
#         pd.Series).stack().reset_index(drop=True)

#     name = ("".join(file.split("_")[:-1]))
#     entries = df.shape[0]
#     sentences = df['sentences'].sum()
#     tokens = df['tokens_len'].sum()
#     vocab = list(set(df['unique_tokens']))
#     vsize = len(set(df['unique_tokens']))

#     names.append(name)
#     entries_ct.append(entries)
#     sent_ct.append(sentences)
#     tok_ct.append(tokens)
#     vocab_list.extend(vocab)
#     vocab_size.append(vsize)

# entries_ct = np.array(entries_ct)
# sent_ct = np.array(sent_ct)
# tok_ct = np.array(tok_ct)
# vocab_size = np.array(vocab_size)

# spe = sent_ct/entries_ct
# tpe = tok_ct/entries_ct
# tps = tok_ct/sent_ct

# ent_tot = entries_ct.sum()
# spe_tot = sent_ct.sum()/entries_ct.sum()
# tpe_tot = tok_ct.sum()/entries_ct.sum()
# tps_tot = tok_ct.sum()/sent_ct.sum()
# vocab_tot = len(set(vocab_list))
# tot_row = np.array(["total", ent_tot, vocab_tot, spe_tot, tpe_tot, tps_tot])

# table = np.vstack((names, entries_ct, vocab_size, spe, tpe, tps)).T
# table = np.vstack((table, tot_row))
# df = pd.DataFrame(table)
# df.columns = ['Subreddit', 'Entries', 'Vocabular Size',
#               'Sentences per Entry', 'Tokens per Entry', 'Tokens per Sentence']
# df.set_index('Subreddit', inplace=True, drop=True)
# df
